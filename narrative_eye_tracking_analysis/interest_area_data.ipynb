{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_QUESTIONS = ['lastPageQuestionsRadio0','lastPageQuestionsRadio1','lastPageQuestionsRadio2','lastPageQuestionsRadio3','lastPageQuestionsRadio4','lastPageQuestionsRadio5','lastPageQuestionsRadio6','lastPageQuestionsRadio7','lastPageQuestionsRadio8']\n",
    "SCALE_QUESTION_TEXT = ['I was curious about what would happen next.',\n",
    "      'The story affected me emotionally.',\n",
    "      'While reading my body was in the room, but my mind was inside the world created by the story.',\n",
    "      'At times while reading, I wanted to know what the writer\\'s intentions were.',\n",
    "      'While reading, when a main character succeeded, I felt happy, and when they suffered in some way, I felt sad.',\n",
    "      'The characters were alive in my imagination.',\n",
    "      'I found my mind wandering while reading the story.',\n",
    "      'I could vividly imagine the scenes in the story.',\n",
    "      'At points, I had a hard time making sense of what was going on in the story.']\n",
    "survey_results = pd.read_csv('survey_results.csv')\n",
    "engagement_total = {'el': 0, 'schoolmistress': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_engagement_score(id, story):\n",
    "  score = 0\n",
    "  row = survey_results[(survey_results['participant_id'] == id) & (survey_results['story'] == story)]\n",
    "  for i in SCALE_QUESTIONS:\n",
    "    if i == 'lastPageQuestionsRadio6' or i == 'lastPageQuestionsRadio8':\n",
    "      score -= (int(row[i]) - 1) # make scale 0 - 4 so that a 1 for negative quesitons does not affect total\n",
    "    else:\n",
    "      score += (int(row[i]) - 1)\n",
    "  engagement_score = (np.round(score / 7, 2)) # divide by number of positive questions\n",
    "  engagement_total[story] += engagement_score\n",
    "  return engagement_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concreteness_score(word):\n",
    "  concreteness_dataset = pd.read_csv('../brysbaert_concreteness_scores_2013.csv')\n",
    "  word_indexes = concreteness_dataset.loc[concreteness_dataset['Word'] == word, 'Conc.M']\n",
    "  return \".\" if len(word_indexes) == 0 else word_indexes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ia_words(story):\n",
    "  with open(f'{story}.txt') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "  words = []\n",
    "  for i in range(len(sentences)):\n",
    "    for word in sentences[i].split(\" \"):\n",
    "      word_cleaned = re.sub(r'[\\\",\\(,\\),\\,,\\;,\\.,\\?,\\!,\\:]', '', word).lower()\n",
    "      words.append({\n",
    "        \"sentence\": i,\n",
    "        \"word\": word_cleaned,\n",
    "        \"word_length\": len(word_cleaned)\n",
    "      })\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_sentences(story):\n",
    "  with open(f'{story}.txt') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "  tokens_df = pd.read_csv(f\"../emotional_story_arcs/data/kelsey/results/{story}.tokens\", sep= '\\t', quoting=3)\n",
    "  words = []\n",
    "  for i in range(len(tokens_df)):\n",
    "    word = tokens_df.iloc[i]\n",
    "    lemma = word['lemma']\n",
    "    if word['POS_tag'] != 'PUNCT':\n",
    "      words.append({\n",
    "        \"sentence\": word['sentence_ID'],\n",
    "        \"word\": lemma,\n",
    "        \"word_length\": len(word['word']),\n",
    "        \"concreteness\": compute_concreteness_score(lemma)\n",
    "      })\n",
    "  return (sentences, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get story text\n",
    "# schoolmistress_sentences, schoolmistress_words = get_story_sentences('schoolmistress')\n",
    "# el_sentences, el_words = get_story_sentences('expensivelessons')\n",
    "# schoolmistress_ia_words = get_ia_words('schoolmistress')\n",
    "# el_ia_words = get_ia_words('expensivelessons')\n",
    "\n",
    "# pd.DataFrame.from_dict(el_words).to_csv(\"el_words.csv\")\n",
    "# pd.DataFrame.from_dict(schoolmistress_words).to_csv(\"schoolmistress_words.csv\")\n",
    "# pd.DataFrame(el_sentences).to_csv(\"el_sentences.csv\")\n",
    "# pd.DataFrame(schoolmistress_sentences).to_csv(\"schoolmistress_sentences.csv\")\n",
    "# pd.DataFrame.from_dict(el_ia_words).to_csv(\"el_ia_words.csv\")\n",
    "# pd.DataFrame.from_dict(schoolmistress_ia_words).to_csv(\"schoolmistress_ia_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_words = pd.read_csv(\"el_words.csv\")\n",
    "schoolmistress_words = pd.read_csv(\"schoolmistress_words.csv\")\n",
    "el_sentences = pd.read_csv(\"el_sentences.csv\")\n",
    "schoolmistress_sentences = pd.read_csv(\"schoolmistress_sentences.csv\")\n",
    "el_ia_words= pd.read_csv(\"el_ia_words.csv\")\n",
    "schoolmistress_ia_words= pd.read_csv(\"schoolmistress_ia_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(file_path):\n",
    "  f = open(file_path, 'r')\n",
    "  dict_list = f.readlines()\n",
    "  return list(map(lambda x: json.loads(x), dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataframe(story, words, sentences):\n",
    "  word_features = pd.DataFrame.from_dict(words)\n",
    "  word_features = word_features[['sentence', 'word_length', 'concreteness']].applymap(lambda x: None if x == '.' else x)\n",
    "  word_features_clean = pd.DataFrame(word_features, dtype='float')\n",
    "  sentence_features = word_features_clean.groupby('sentence').agg({'word_length': 'mean', 'concreteness': 'mean'})\n",
    "  # get book nlp features\n",
    "  emotion_json = convert_to_json(f'../emotional_story_arcs/data/kelsey/results/{story}.emotion')\n",
    "  features_df = pd.concat([pd.DataFrame.from_dict(emotion_json), \n",
    "              pd.read_csv(f'../emotional_story_arcs/data/kelsey/results/{story}.sentiment', sep='\\t', names=['negative', 'neutral', 'positive'])], axis=1)\n",
    "  features_df = pd.concat([features_df, sentence_features], axis=1)\n",
    "  return (pd.concat([features_df, sentences], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs(filename, features, words):\n",
    "  # append story text to eyelink file\n",
    "  eyelink_data = pd.read_csv(f'ia_files/{filename}.txt', sep='\\t', low_memory=False)\n",
    "  eyelink_data2 = eyelink_data\n",
    "  words_df = pd.DataFrame(words)\n",
    "  ia_df_subset = eyelink_data[['IA_DWELL_TIME', 'IA_REGRESSION_PATH_DURATION', 'IA_AVERAGE_FIX_PUPIL_SIZE', \n",
    "        'IA_REGRESSION_IN_COUNT','IA_REGRESSION_OUT_FULL_COUNT']] \\\n",
    "        .applymap(lambda x: None if x == '.' else x)\n",
    "  ia_df_clean = pd.DataFrame(ia_df_subset, dtype='float')\n",
    "  eyelink_with_text = pd.concat([words_df, ia_df_clean], axis=1)\n",
    "  # aggregate columns to get sentence vals\n",
    "  ia_sentences_df = eyelink_with_text.groupby('sentence')\\\n",
    "      .agg({ 'IA_DWELL_TIME': 'sum', 'IA_REGRESSION_PATH_DURATION': 'sum',\n",
    "                    'IA_AVERAGE_FIX_PUPIL_SIZE': 'mean', 'IA_REGRESSION_IN_COUNT': 'sum',\n",
    "                    'IA_REGRESSION_OUT_FULL_COUNT': 'sum', 'word': 'count'})\n",
    "  # Get highlight categories\n",
    "  highlights = pd.read_csv(f'./highlights/{filename}.csv').drop(['Unnamed: 0', 'proportion'], axis=1)\n",
    "  # Story features + some eye tracking features in one DataFrame by sentence\n",
    "  sentences_with_features = pd.concat([eyelink_data2[['RECORDING_SESSION_LABEL']][0:len(ia_sentences_df)], ia_sentences_df], axis=1)\n",
    "  sentences_with_features = pd.concat([highlights, sentences_with_features], axis=1)\n",
    "  sentences_with_features = pd.concat([sentences_with_features, features], axis=1)\n",
    "  sentences_with_features['engagement_score'] = [compute_engagement_score(filename.split('_')[0], filename.split('_')[1].replace('.txt', ''))] * len(sentences_with_features)\n",
    "  sentences_with_features.to_csv(f\"./results/{filename}.csv\", )\n",
    "  return sentences_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expensive Lessons avg score: 2.09, schoolmistress avg score: 1.94\n"
     ]
    }
   ],
   "source": [
    "IA_DIR = \"./ia_files/\"\n",
    "schoolmistress_features = get_features_dataframe('schoolmistress', schoolmistress_words, schoolmistress_sentences)\n",
    "el_features = get_features_dataframe('expensivelessons', el_words, el_sentences)\n",
    "for filename in os.listdir(IA_DIR):\n",
    "  f = os.path.join(IA_DIR,filename)\n",
    "  if os.path.isfile(f):\n",
    "      filename = f.replace(IA_DIR, '').replace('.txt', '')\n",
    "      if 'schoolmistress' in filename:\n",
    "        process_logs(filename, schoolmistress_features, schoolmistress_ia_words)\n",
    "      elif 'el' in filename:\n",
    "        process_logs(filename, el_features, el_ia_words)\n",
    "\n",
    "print(f\"Expensive Lessons avg score: {np.round(engagement_total['el'] / 23, 2)}, schoolmistress avg score: {np.round(engagement_total['schoolmistress'] / 23, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values\n",
    "result_df = pd.read_csv(\"./results/id11_el.csv\")\n",
    "# fill_vals = {'IA_DWELL_TIME': 0, 'IA_REGRESSION_PATH_DURATION': 0,\n",
    "#         'IA_AVERAGE_FIX_PUPIL_SIZE': result_df['IA_AVERAGE_FIX_PUPIL_SIZE'].mean(),\n",
    "#         'IA_REGRESSION_IN_COUNT': 0, 'IA_REGRESSION_OUT_COUNT': 0}\n",
    "        \n",
    "fill_vals = {'IA_DWELL_TIME': 'empty', 'IA_REGRESSION_PATH_DURATION': 'empty',\n",
    "        'IA_AVERAGE_FIX_PUPIL_SIZE': 'empty',\n",
    "        'IA_REGRESSION_IN_COUNT': 'empty', 'IA_REGRESSION_OUT_FULL_COUNT': 'empty'}\n",
    "result_df = result_df.fillna(value=fill_vals)\n",
    "result_df = result_df[(result_df['IA_DWELL_TIME'] == 'empty') | \\\n",
    "         (result_df['IA_REGRESSION_PATH_DURATION'] == 'empty') | \\\n",
    "          (result_df['IA_AVERAGE_FIX_PUPIL_SIZE'] == 'empty') | \\\n",
    "            (result_df['IA_REGRESSION_IN_COUNT'] == 'empty') | \\\n",
    "            (result_df['IA_REGRESSION_OUT_FULL_COUNT'] == 'empty')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_empties(df):\n",
    "  fill_vals = {'IA_DWELL_TIME': 'empty', 'IA_REGRESSION_PATH_DURATION': 'empty',\n",
    "        'IA_AVERAGE_FIX_PUPIL_SIZE': 'empty',\n",
    "        'IA_REGRESSION_IN_COUNT': 'empty', 'IA_REGRESSION_OUT_FULL_COUNT': 'empty'}\n",
    "  return df.fillna(value=fill_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_report = result_df\n",
    "for filename in os.listdir('./results'):\n",
    "    f = os.path.join('./results',filename)\n",
    "    if os.path.isfile(f) and f != './results/id11_el.csv':\n",
    "      curr_df = populate_empties(pd.read_csv(f))\n",
    "      curr_empties = curr_df[(curr_df['IA_DWELL_TIME'] == 'empty') | \\\n",
    "         (curr_df['IA_REGRESSION_PATH_DURATION'] == 'empty') | \\\n",
    "          (curr_df['IA_AVERAGE_FIX_PUPIL_SIZE'] == 'empty') | \\\n",
    "            (curr_df['IA_REGRESSION_IN_COUNT'] == 'empty') | \\\n",
    "            (curr_df['IA_REGRESSION_OUT_FULL_COUNT'] == 'empty')\n",
    "        ]\n",
    "      empty_report = pd.concat([empty_report, curr_empties], axis=0)\n",
    "\n",
    "empty_report['tally'] = [1]*len(empty_report)\n",
    "empty_report.to_csv(\"empty_rows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_empty_report = empty_report.groupby('RECORDING_SESSION_LABEL').agg({\"tally\": 'sum'})\n",
    "grouped_empty_report.to_csv(\"grouped_empty_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('narrative_engagement-1kgk4Qlh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce6e9f89b8ba06df44163b972ee2537d17b2ddac401432716e00090db9961c19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
