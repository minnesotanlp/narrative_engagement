{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concreteness_score(word):\n",
    "  concreteness_dataset = pd.read_csv('../brysbaert_concreteness_scores_2013.csv')\n",
    "  word_indexes = concreteness_dataset.loc[concreteness_dataset['Word'] == word, 'Conc.M']\n",
    "  return \".\" if len(word_indexes) == 0 else word_indexes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ia_words(story):\n",
    "  with open(f'{story}.txt') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "  words = []\n",
    "  for i in range(len(sentences)):\n",
    "    for word in sentences[i].split(\" \"):\n",
    "      word_cleaned = re.sub(r'[\\\",\\(,\\),\\,,\\;,\\.,\\?,\\!,\\:]', '', word).lower()\n",
    "      words.append({\n",
    "        \"sentence\": i,\n",
    "        \"word\": word_cleaned,\n",
    "        \"word_length\": len(word_cleaned)\n",
    "      })\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_sentences(story):\n",
    "  with open(f'{story}.txt') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "  tokens_df = pd.read_csv(f\"../emotional_story_arcs/data/kelsey/results/{story}.tokens\", sep= '\\t', quoting=3)\n",
    "  words = []\n",
    "  for i in range(len(tokens_df)):\n",
    "    word = tokens_df.iloc[i]\n",
    "    lemma = word['lemma']\n",
    "    if word['POS_tag'] != 'PUNCT':\n",
    "      words.append({\n",
    "        \"sentence\": word['sentence_ID'],\n",
    "        \"word\": lemma,\n",
    "        \"word_length\": len(word['word']),\n",
    "        \"concreteness\": compute_concreteness_score(lemma)\n",
    "      })\n",
    "  return (sentences, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get story text\n",
    "# schoolmistress_sentences, schoolmistress_words = get_story_sentences('schoolmistress')\n",
    "# el_sentences, el_words = get_story_sentences('expensivelessons')\n",
    "# schoolmistress_ia_words = get_ia_words('schoolmistress')\n",
    "# el_ia_words = get_ia_words('expensivelessons')\n",
    "\n",
    "# pd.DataFrame.from_dict(el_words).to_csv(\"el_words.csv\")\n",
    "# pd.DataFrame.from_dict(schoolmistress_words).to_csv(\"schoolmistress_words.csv\")\n",
    "# pd.DataFrame(el_sentences).to_csv(\"el_sentences.csv\")\n",
    "# pd.DataFrame(schoolmistress_sentences).to_csv(\"schoolmistress_sentences.csv\")\n",
    "# pd.DataFrame.from_dict(el_ia_words).to_csv(\"el_ia_words.csv\")\n",
    "# pd.DataFrame.from_dict(schoolmistress_ia_words).to_csv(\"schoolmistress_ia_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_words = pd.read_csv(\"el_words.csv\")\n",
    "schoolmistress_words = pd.read_csv(\"schoolmistress_words.csv\")\n",
    "el_sentences = pd.read_csv(\"el_sentences.csv\")\n",
    "schoolmistress_sentences = pd.read_csv(\"schoolmistress_sentences.csv\")\n",
    "el_ia_words= pd.read_csv(\"el_ia_words.csv\")\n",
    "schoolmistress_ia_words= pd.read_csv(\"schoolmistress_ia_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(file_path):\n",
    "  f = open(file_path, 'r')\n",
    "  dict_list = f.readlines()\n",
    "  return list(map(lambda x: json.loads(x), dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataframe(story, words, sentences):\n",
    "  word_features = pd.DataFrame.from_dict(words)\n",
    "  word_features = word_features[['sentence', 'word_length', 'concreteness']].applymap(lambda x: None if x == '.' else x)\n",
    "  word_features_clean = pd.DataFrame(word_features, dtype='float')\n",
    "  sentence_features = word_features_clean.groupby('sentence').agg({'word_length': 'mean', 'concreteness': 'mean'})\n",
    "  # get book nlp features\n",
    "  emotion_json = convert_to_json(f'../emotional_story_arcs/data/kelsey/results/{story}.emotion')\n",
    "  features_df = pd.concat([pd.DataFrame.from_dict(emotion_json), \n",
    "              pd.read_csv(f'../emotional_story_arcs/data/kelsey/results/{story}.sentiment', sep='\\t', names=['negative', 'neutral', 'positive'])], axis=1)\n",
    "  features_df = pd.concat([features_df, sentence_features], axis=1)\n",
    "  return (pd.concat([features_df, sentences], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs(filename, features, words):\n",
    "  # append story text to eyelink file\n",
    "  eyelink_data = pd.read_csv(f'ia_files/{filename}.txt', sep='\\t', low_memory=False)\n",
    "  eyelink_data2 = eyelink_data\n",
    "  words_df = pd.DataFrame(words)\n",
    "  ia_df_subset = eyelink_data[['IA_DWELL_TIME', 'IA_REGRESSION_PATH_DURATION', 'IA_AVERAGE_FIX_PUPIL_SIZE', \n",
    "        'IA_REGRESSION_IN_COUNT','IA_REGRESSION_OUT_FULL_COUNT']] \\\n",
    "        .applymap(lambda x: None if x == '.' else x)\n",
    "  ia_df_clean = pd.DataFrame(ia_df_subset, dtype='float')\n",
    "  eyelink_with_text = pd.concat([words_df, ia_df_clean], axis=1)\n",
    "  # aggregate columns to get sentence vals\n",
    "  ia_sentences_df = eyelink_with_text.groupby('sentence')\\\n",
    "      .agg({ 'IA_DWELL_TIME': 'sum', 'IA_REGRESSION_PATH_DURATION': 'sum',\n",
    "                    'IA_AVERAGE_FIX_PUPIL_SIZE': 'mean', 'IA_REGRESSION_IN_COUNT': 'sum',\n",
    "                    'IA_REGRESSION_OUT_FULL_COUNT': 'sum', 'word': 'count'})\n",
    "  # Get highlight categories\n",
    "  highlights = pd.read_csv(f'./highlights/{filename}.csv').drop(['Unnamed: 0', 'proportion'], axis=1)\n",
    "  # Story features + some eye tracking features in one DataFrame by sentence\n",
    "  sentences_with_features = pd.concat([eyelink_data2[['RECORDING_SESSION_LABEL']][0:len(ia_sentences_df)], ia_sentences_df], axis=1)\n",
    "  sentences_with_features = pd.concat([highlights, sentences_with_features], axis=1)\n",
    "  sentences_with_features = pd.concat([sentences_with_features, features], axis=1)\n",
    "  sentences_with_features.to_csv(f\"./results/{filename}.csv\", )\n",
    "  return sentences_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "IA_DIR = \"./ia_files/\"\n",
    "schoolmistress_features = get_features_dataframe('schoolmistress', schoolmistress_words, schoolmistress_sentences)\n",
    "el_features = get_features_dataframe('expensivelessons', el_words, el_sentences)\n",
    "for filename in os.listdir(IA_DIR):\n",
    "    f = os.path.join(IA_DIR,filename)\n",
    "    if os.path.isfile(f):\n",
    "        filename = f.replace(IA_DIR, '').replace('.txt', '')\n",
    "        if 'schoolmistress' in filename:\n",
    "          process_logs(filename, schoolmistress_features, schoolmistress_ia_words)\n",
    "        elif 'el' in filename:\n",
    "          process_logs(filename, el_features, el_ia_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values\n",
    "result_df = pd.read_csv(\"./results/id11_el.csv\")\n",
    "# fill_vals = {'IA_DWELL_TIME': 0, 'IA_REGRESSION_PATH_DURATION': 0,\n",
    "#         'IA_AVERAGE_FIX_PUPIL_SIZE': result_df['IA_AVERAGE_FIX_PUPIL_SIZE'].mean(),\n",
    "#         'IA_REGRESSION_IN_COUNT': 0, 'IA_REGRESSION_OUT_COUNT': 0}\n",
    "        \n",
    "fill_vals = {'IA_DWELL_TIME': 'empty', 'IA_REGRESSION_PATH_DURATION': 'empty',\n",
    "        'IA_AVERAGE_FIX_PUPIL_SIZE': 'empty',\n",
    "        'IA_REGRESSION_IN_COUNT': 'empty', 'IA_REGRESSION_OUT_FULL_COUNT': 'empty'}\n",
    "result_df = result_df.fillna(value=fill_vals)\n",
    "result_df = result_df[(result_df['IA_DWELL_TIME'] == 'empty') | \\\n",
    "         (result_df['IA_REGRESSION_PATH_DURATION'] == 'empty') | \\\n",
    "          (result_df['IA_AVERAGE_FIX_PUPIL_SIZE'] == 'empty') | \\\n",
    "            (result_df['IA_REGRESSION_IN_COUNT'] == 'empty') | \\\n",
    "            (result_df['IA_REGRESSION_OUT_FULL_COUNT'] == 'empty')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_empties(df):\n",
    "  fill_vals = {'IA_DWELL_TIME': 'empty', 'IA_REGRESSION_PATH_DURATION': 'empty',\n",
    "        'IA_AVERAGE_FIX_PUPIL_SIZE': 'empty',\n",
    "        'IA_REGRESSION_IN_COUNT': 'empty', 'IA_REGRESSION_OUT_FULL_COUNT': 'empty'}\n",
    "  return df.fillna(value=fill_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_report = result_df\n",
    "for filename in os.listdir('./results'):\n",
    "    f = os.path.join('./results',filename)\n",
    "    if os.path.isfile(f) and f != './results/id11_el.csv':\n",
    "      curr_df = populate_empties(pd.read_csv(f))\n",
    "      curr_empties = curr_df[(curr_df['IA_DWELL_TIME'] == 'empty') | \\\n",
    "         (curr_df['IA_REGRESSION_PATH_DURATION'] == 'empty') | \\\n",
    "          (curr_df['IA_AVERAGE_FIX_PUPIL_SIZE'] == 'empty') | \\\n",
    "            (curr_df['IA_REGRESSION_IN_COUNT'] == 'empty') | \\\n",
    "            (curr_df['IA_REGRESSION_OUT_FULL_COUNT'] == 'empty')\n",
    "        ]\n",
    "      empty_report = pd.concat([empty_report, curr_empties], axis=0)\n",
    "\n",
    "empty_report.to_csv(\"empty_rows.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x1773433a0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_report.groupby('RECORDING_SESSION_LABEL').agg({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-scraps-fAaJKGr8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581f1045db2280ff6b67f7603a6643419e6d4ff8b0ea51ff4bcc2bb23747725a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
